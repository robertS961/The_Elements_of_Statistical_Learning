{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2 Exercises"
      ],
      "metadata": {
        "id": "xQpwhWz6yfWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex. 2.1 Suppose each of K-classes has an associated target $t_k$, which is a vector of all zeros, except a one in the $k_{th}$ position. Show that classifying to the largest element of $\\hat{y}$ amounts to choosing the closest target, $\\text{min} =  ||t_k − \\hat{y}||$, if the elements of $\\hat{y}$ sum to one."
      ],
      "metadata": {
        "id": "rqDQstGv0IJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution 2.1\n",
        "\n",
        "$$\\text{min}_k ||t_k − \\hat{y}|| = \\text{argmin}_k \\sum _{i=1}^{K} (t_{ki} − \\hat{y_i})^2 $$\n",
        "\n",
        "$$ = \\text{argmin}_k \\sum _{i=1}^{K} (t_{ki}^2 + \\hat{y}_i^2 - 2 t_{ki}\\hat{y}_i)$$\n",
        "\n",
        "### Note\n",
        "\n",
        "(1) $$  t_{ki}^2 = 1  $$\n",
        "(2) $$ \\hat{y}_i^2 = c $$\n",
        "(3) $$2 t_{ki}\\hat{y}_i = 2\\hat{y}_i$$\n",
        "\n",
        "> Using 1,2,3 then gives $$ \\text{argmin}_k (1 + c - 2 \\hat{y}_i)$$\n",
        "\n",
        "$$ = \\text{argmin}_k (-2 \\hat{y}_i)$$\n",
        "\n",
        "$$ = \\text{argmax}_y (\\hat{y}_i)$$\n",
        "\n",
        "* Thus we pick the max from $\\hat{y}$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ZF4eYFg0pt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex. 2.2 Show how to compute the Bayes decision boundary for the simulation example in Figure 2.5."
      ],
      "metadata": {
        "id": "MYLrEcyI2wXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution 2.2\n",
        "\n",
        "$$ O ∈ N(m_0, I/5)$$\n",
        "$$m_o \\in N((0,1)^T, I)$$\n",
        "\n",
        "$$ B ∈ N(m_b, I/5)$$\n",
        "$$m_b \\in N((1,0)^T, I)$$\n",
        "\n",
        "* We are given 100 $O$ and 100 $B$. There are 10 $m_o$ and 10 $m_b$ generated. So $\\frac {1}{10}$ to pick each time a $B$ or $O$ is generated.\n",
        "\n",
        "* We use maximal likelihood to calculate the Bayes decision boundary. We do this since Bayes is defined by $$ P(G_K | X = x) = \\text{max}_{g \\in G} P(g| X = X)$$\n",
        "\n",
        "* Thus we need to define $P(g|X=x)$ which is a conditional probability. Thus using maximum likelihood.\n",
        "\n",
        "$$ P(g | X = x) = \\frac { 1} {\\sqrt{2πσ_x}} \\exp{\\frac{-(g- μ_x)^2}{2σ_x^2}}$$\n",
        "\n",
        "* Insert in the $μ_x$, $g_O$, $g_B$ from above along with $σ_x^2$\n",
        "\n",
        "$$ \\text{orange} = \\frac { 1} {\\sqrt{2π5^{-1}}} \\exp{\\sum_{g \\in O}^{10} \\sum_{o =1}^{10} \\frac{-5(g- m_o)^2}{2}}$$\n",
        "\n",
        "$$ \\text{blue} = \\frac { 1} {\\sqrt{2π5^{-1}}} \\exp{ \\sum_{g \\in B}^{10} \\sum_{b =1}^{10} \\frac{-5(g- m_b)^2}{2}}$$\n",
        "\n",
        "$$ \\text{orange} = \\text{blue}$$\n",
        "\n",
        "* We need the decision boundary to always be at $P(.50)$ to make decisions about which color goes where.\n",
        "\n",
        "* Thus $$\\frac{1}{2} = \\frac{1}{2} = \\text{orange} = \\text{blue}$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gx9NciFa21Cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex. 2.3 Derive equation (2.24)."
      ],
      "metadata": {
        "id": "17UuxNpO_jeE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ixUH6YaCdvI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}