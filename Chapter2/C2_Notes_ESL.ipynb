{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2"
      ],
      "metadata": {
        "id": "mZ5oqlrXxtVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Introduction\n",
        "\n",
        "- Normally given inputs and outputs. Our goal is to use the inputs to predict the outputs\n",
        "\n",
        "- Inputs are more commonly referred to as the predictors or independent variables.\n",
        "\n",
        "- The response is commonly referred to as the output or dependent variables\n",
        "\n",
        "- This setup is called supervised learning since we have an output to our features."
      ],
      "metadata": {
        "id": "V_aK-Nx8xu_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Variable Type and Terminology\n",
        "\n",
        "- Quantitative variables like numbers\n",
        "\n",
        "- Qualitative variables like categories without a specific ordering.\n",
        "\n",
        "- Regression is when we predict quantitative variables for the output\n",
        "\n",
        "- Classification is when we predict qualitative variables for the output\n",
        "\n",
        "- Last type is Ordered Categorical where there is an ordering to the categories like small, medium, and large. Of course small is closer to medium than large so the ordering matters.\n",
        "\n",
        "- Dealing with Categorical Inputs: if only two options can use a binary coding style $(0,1)$. If there is $K$ options then can do dummy variables. This is a vector of $K$ 0's with one 1 where the variable is true.\n",
        "\n",
        "- X normally represents the input, Y represents the output, G if the output is qualitative.\n",
        "\n",
        "- Observed values will be written in lower case such as $x_i$. This refers to a column of X. It's length is $N$.\n",
        "\n",
        "- Matrix will be represented by bold, capital letters like **X**. They will be denoted by Row x Columns. So a set of $N$ input  $p$ vectors $x_i = 1,....,N$ would be $ N ùöá p$.\n",
        "\n",
        "- All vectors are assumed to be column vectors coming from the p predictors. Hence if we want to refer to a row vector we would need to transpose the matrix.\n",
        "\n",
        "- Thus we now can make predictors on our input data to replicate our output data. Our predicted output is called $YÃÇ$ or $GÃÇ$ for categorical variables.\n",
        "\n",
        "- IF there data has two output classes we can break then down to binary and pick a threshold value to split the variables into $G_1$ or $G_2$ like $y<0.5$"
      ],
      "metadata": {
        "id": "-k42qvj3ygTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Two Simple Approaches to Prediction: Least Squares and K Closest Neighbors.\n",
        "\n",
        "- Least Squares: Stable but can be inaccurate\n",
        "\n",
        "- K Closest Neighbors: Unstable but can be extremely accurate"
      ],
      "metadata": {
        "id": "TYbRCZxw4LU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1 Linear Model and Least Squares\n",
        "\n",
        "- The term $B_0$ is know as the intercept but in machine learning it is called the bias. If it is not included then it cuts the y axis at the origin.\n",
        "\n",
        "- Can write it as: $$ \\widehat{Y} = \\widehat{B_{0}} + \\sum_{j=1}^p X_j \\widehat{B_j}$$\n",
        "\n",
        "-Assume the X = 1 and include $B_0$ then you can write it as an inner product $$ \\hat{Y} = X^T \\hat{B}$$\n",
        "\n",
        "- Thus $f'(X) = B$ the steepest gradient possible.\n",
        "\n"
      ],
      "metadata": {
        "id": "YnE1y_xk5ODQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Least Squares\n",
        "\n",
        "- Goal is to minimize the RSS(Residual Sum of Squares). $$\\text{RSS(B)} = \\sum_{i=1}^{N}(y_i - x_i^TB)^2 $$ This is a quadradic function and can be solved for a minimum.\n",
        "\n",
        "- Another way to write it is in Matrix Notation. $$\\text{RSS(B)} = (Y -XB)^T(Y-XB)$$\n",
        "\n",
        "- Differienate with regards to $B$ $$ X^T(y - XB) = 0$$\n",
        "\n",
        "- If $X^TX$ is nonsingular then $$B = (X^TX)^{-1}-X^Ty$$"
      ],
      "metadata": {
        "id": "nTjGBcOVG12k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K Nearest Neighbors Methods\n",
        "\n",
        "- $$ \\hat{Y}(x) = \\sum_{x_i ‚àà N_k(x)} y_i$$ where $N_k$ is the neighborhood of $k$ closest $x_i$ points.\n",
        "\n",
        "- So we use Euclidean distance on $k$ points that are closest to $x$ then average them.\n",
        "\n",
        "- With regression we pick $p$ parameters but with k nearest neighbors we pick $k$ neighbors.\n",
        "\n",
        "- k = 1 will always give us the best value on the training data. However the optimal value of $k$ is normally around $N\\k > p$. Cross Validation is a great way to find the most optimal $k$."
      ],
      "metadata": {
        "id": "ULAL9BLnEO6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.2 From Least Squares to K Nearest Neighbors\n",
        "\n",
        "- Least squares has a stable decision boundary that is based on all the points. Therefore it has low variance but a high bias. However it has an assumption that the boundary line is linear.\n",
        "\n",
        "- K Nearest Neighbor decision boundary relies on only a handful of points $k$. Therefore it has high variance but low bias. It is much more unstable. No assumption here as the boundary line can easily change based on $k$.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "fzP-rWFAGqyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alternative to KNN\n",
        "\n",
        "- K Nearest Neighbor has more complex methods that build off it. Such as kernel methods that decrease smoothly to zero the futher away the point is rather than the (0,1) value from KNN.\n",
        "\n",
        "- In high dimensional space the Kernels are modified to emphasis certain variables more than others."
      ],
      "metadata": {
        "id": "kurWnlq9G6Lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alternative to Least Squares\n",
        "\n",
        "- Local regression fits models to locally weighted least squares instead of constant local fit.\n",
        "\n",
        "- Linear Model fit to a basis expansion of the original variables\n",
        "\n",
        "- Projection and Neural Networks consist of sums of nonlinear transformed linear models"
      ],
      "metadata": {
        "id": "WeZBy9mjJ-TB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Statistical Decision Theory\n",
        "\n",
        "- K Nearest Neighbors can provide an amazing approximation if N is extremely large. However, we usually don't have enough data for this and settle for a regression approach.\n",
        "\n",
        "- Also if P is too large , KNN fails also as there are too many neighborhoods for the local points.\n",
        "\n",
        "- KNN neighbor assume an average based on local points while linear regression assumes an average based on global points.\n",
        "\n",
        "- Learn the best method for classification is the Bayes decision boundary. It uses condition probability to classify the point. We see KNN does a relaxed version of this by using the class of the points around it."
      ],
      "metadata": {
        "id": "mp06IlJjKhKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Local Methods in High Dimensions\n",
        "\n",
        "- The two algorithms learned so far break down with high dimensionality. Aka large values of p compared to the amount of data.\n",
        "\n",
        "- The problem is that with this many dimensions, our neighborhood became extremely vast over the space. They are no longer local. We can try to reduce their k but then our variance drastically spikes.\n",
        "\n",
        "- Another problem is most points fall closer to the edges than to the center. Predicting results near the edge is more difficult than predicting them around the center.\n",
        "\n",
        "- Also as dimensions get higher, it is extremely difficult to keep the same proportion of sampling density. It is exponential.\n",
        "\n",
        "- We derived the MSE = Variance + $\\text{Bias}^2$. By\n",
        "- 1. Adding in the $ - E(\\hat{y}) +  E(\\hat{y})$.\n",
        "- 2. Factoring\n",
        "- 3. Eliminating terms by Expectation of Linearity\n",
        "- 4. Reducing to Variance and $\\text{Bias}^2$\n",
        "\n",
        "- The key to curing the curse of high dimensionality is extremely large $N$ sample size or extremely small variance $œÉ^2$. This is because the equation for Expected Prediction Error is $$œÉ^2 p /N$$\n",
        "\n"
      ],
      "metadata": {
        "id": "dtTKGrxyRepX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Statistical Models, Supervised Learning, and Function Approximations\n",
        "\n",
        "* Can use other functions if they specifically describe the data. Especially if it is in high dimension"
      ],
      "metadata": {
        "id": "-BcRIKW0VJih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.1 A Statistical Model for Joint Distribution $Pr(X,Y)$\n",
        "\n",
        "* Additive model incorporates error, assumes the error estimate is $E(e) = 0$, and is independent\n",
        "\n",
        "$$ Y = f(X) + e$$"
      ],
      "metadata": {
        "id": "BpZykdNWFCvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.2 Supervised Learning\n",
        "\n",
        "* Learn by examples like using least squares\n",
        "\n",
        "* Learns from the input/output example\n"
      ],
      "metadata": {
        "id": "H-no3Wqo5URn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.3 Function Approximation\n",
        "\n",
        "* Linear Basis Expansion where we use multiple functions to describe our model\n",
        "\n",
        "$$ f_{\\theta}(x) = \\sum_{k=1}^K h_k(x) \\theta_k$$\n",
        "\n",
        "* Where $h_k$ could be any function, one common one is the sigmoid transformation.\n",
        "\n",
        "$$ h_k(x) = \\frac {1}{1 + \\exp{-x^TB_k}}$$\n",
        "\n",
        "* We can use RSS on this going through each point for each function.\n",
        "\n",
        "$$RSS(\\theta) = \\sum_{i=1}^N (y - f_{\\theta}(x_i))^2$$\n",
        "\n",
        "* In Some cases RSS doesn't make sense maybe for a classification problem or a density problem. In these cases we would use maximal likelihood estimator.\n",
        "\n",
        "$$L(\\theta) = \\sum_{i=1}^N \\text{log}Pr_{\\theta}(y_i) $$"
      ],
      "metadata": {
        "id": "n3JaGNOv5x89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Structure Regression Models\n",
        "\n",
        "* Might not want an extremely flexible method if there can be a more accurate structured rigid method\n",
        "\n",
        "* We might want this especially do this in high dimensions and sometimes in low dimensions."
      ],
      "metadata": {
        "id": "qPCcfdLz7PMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.1 Difficulty of the Problem\n",
        "\n",
        "* We might sometimes want to reshape the problem to make it more accurate. In this case we would eliminate or change constraints which makes another variable to change.\n",
        "\n",
        "* To fix this we develop models that look for neighborhoods that are extremely dense and have a large number of close points.\n",
        "\n",
        "* This works well and even in high dimensions if the variance is small enough and the sample size is large enough.\n",
        "\n",
        "* Our goal in this book is to moderate the inputs using these different parameters to make accurate models. However to find these neighborhoods we need tight, dense, high N areas."
      ],
      "metadata": {
        "id": "k2o3SgEy-YdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Class of Restricted Estimators\n",
        "\n",
        "* These classes are not mutually exclusive\n",
        "\n",
        "* They are ways to capture or categorize the neighborhoods"
      ],
      "metadata": {
        "id": "PNI6kaAC_Tro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8.1 Roughness Penalty and Bayesian Methods\n",
        "\n",
        "* We penalize the RSS with a roughness score. The rougher the curve the more penalty is applied\n",
        "\n",
        "$$ \\text{PRSS}(f ; \\lambda) = RSS(f) + \\lambda J(f) $$\n",
        "\n",
        "$$ \\text{PRSS}(f ; \\lambda) = \\sum_{i=1}^N (y - f(x_i))^2 + \\lambda \\int [f^{''}(x)]^2 dx $$"
      ],
      "metadata": {
        "id": "ypC1NrJl_t2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8.2 Kernel methods and Local Regression\n",
        "\n",
        "* The local Neighborhood is defined by a Kernel function. It assigns weight to the local points.\n",
        "\n",
        "$$ K_{\\lambda}(x_0, x) = \\frac{1}{\\lambda} \\exp{-\\frac{||x - x_0||^2}{2 \\lambda}}$$\n",
        "\n",
        "* The simplest form of kernel density is Nadaraya - Watson\n",
        "\n",
        "$$\\hat{f(x_0)} =  \\frac{\\sum_{i=1}^NK_{\\lambda}(x_0, x_i) (y_i)}{\\sum_{i=1}^NK_{\\lambda}(x_0, x_i)} $$\n",
        "\n",
        "* Define a local regression estimate as\n",
        "\n",
        "$$RSS(f_\\theta, x_0) = \\sum_{i=1}^N K_{\\lambda}(x_0, x_i)(y - f_{\\theta}(x_i))^2 $$\n",
        "\n",
        " $$f_{\\theta}(x) = \\theta _0$$ we get Nadaraya equation\n",
        "\n",
        " $$ f_{\\theta}(x) = \\theta _0 + \\theta_1 (x)$$ gives us the RSS equation\n"
      ],
      "metadata": {
        "id": "caX3ZaRKBDeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8.3 Basis Function and Dictionary Methods\n",
        "\n",
        "* Use polynomials, and a variety of flexible methods.\n",
        "\n",
        "$$ f_{\\theta}(x) = \\sum_{k=1}^K h_k(x) \\theta_k$$\n",
        "\n",
        "* These methods are called linear expansion of basis functions. Called linear since $\\theta$ is some form of linear function.\n",
        "\n",
        "* Can use splines with the above function utilizing piece-wise functions. With knots connecting them.\n",
        "\n",
        "* Radial basis function are symmetric p dimensional kernels located at particular centroids.\n",
        "\n",
        "$$ f_{\\theta}(x) = \\sum_{m=1}^M K_{\\lambda_m}(\\mu_m, x) \\theta_m$$\n",
        "\n",
        "* You could use alternative kernels here.\n",
        "\n",
        "* Lastly we can project instead of shrink with an adaptive basis function\n",
        "\n",
        "$$f_{\\theta}(x) = \\sum_{m=1}^M B_m \\sigma (\\alpha_m^Tx + b_m) $$\n",
        "\n",
        "* Where $\\alpha$ is the projection\n",
        "\n"
      ],
      "metadata": {
        "id": "i3lhXhLGEqw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.9 Model Selection and Bias-Variance Trade off\n",
        "\n",
        "* Difficult to pick the correct parameters for a model like the $\\lambda $ value to use, width of the kernel, or the number of basis functions to use\n",
        "\n",
        "* Also difficult how much to the train the data on the training set. This is where bias vs variance trade off comes in.\n",
        "\n",
        "* The more we train our model the closer average of our values become to the predicted values. Thus lowering bias.\n",
        "\n",
        "* However further away our predictions become from our average.\n",
        "\n",
        "* Must find the perfect balance of the bias-variance trade off"
      ],
      "metadata": {
        "id": "TvHCm8iaE-aY"
      }
    }
  ]
}